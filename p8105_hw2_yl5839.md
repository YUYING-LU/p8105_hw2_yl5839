p8105_hw2_yl5839
================
Yuying Lu
2024-09-29

Import necessary packages

``` r
library(tidyverse)
library(readxl)
```

# Problem 1

## Data Importing and Cleaning

``` r
df_nyc_trans = read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |> 
  janitor::clean_names() |> 
  select(c('line', 'station_name', 'station_latitude', 'station_longitude', 'route1', 'route2', 'route3', 'route4', 'route5','route6', 'route7', 'route8', 'route9', 'route10', 'route11', 'entry', 'vending', 'entrance_type', 'ada')) |> 
  mutate(entry = ifelse(entry=='YES', TRUE, FALSE),
         vending = ifelse(vending=='YES', TRUE, FALSE))
head(df_nyc_trans,5)
```

    ## # A tibble: 5 × 19
    ##   line     station_name station_latitude station_longitude route1 route2 route3
    ##   <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
    ## 1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ## 2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ## 3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ## 4 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ## 5 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ## # ℹ 12 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
    ## #   route8 <dbl>, route9 <dbl>, route10 <dbl>, route11 <dbl>, entry <lgl>,
    ## #   vending <lgl>, entrance_type <chr>, ada <lgl>

### Description

The NYC transit contains information related to each entrance and exit
for each subway station in NYC, such as the line, station, name, station
latitude or longitude, routes served, entry, vending, entrance type, and
ADA compliance.

I clean the data by first clean the column names using
`janitor::clean_names()` and then select the useful columns I need via
`select()`. After that I convert (`YES` or `NO`) in column `entry` and
`vending` to logical variable `TRUE` or `FALSE` using `ifelse()`.
Finally I got the resulting data with 19 columns and 1868 rows.

### Answer

- There are totally 465 distinct stations;
- There are 468 stations are ADA compliant;
- There are 9.797% of entrances/exits without vending allow entrance.

### Reformat Data

Reform the data with route number and route name being distinct
variables.

``` r
df_re = df_nyc_trans |> 
  mutate(route8=as.character(route8),
         route9=as.character(route9),
         route10=as.character(route10),
         route11=as.character(route11),) |> 
  pivot_longer(route1:route11,
               names_to = 'route_number',
               values_to = 'route_name') |> 
  select(route_number, route_name, everything())
  
head(df_re)
```

    ## # A tibble: 6 × 10
    ##   route_number route_name line   station_name station_latitude station_longitude
    ##   <chr>        <chr>      <chr>  <chr>                   <dbl>             <dbl>
    ## 1 route1       R          4 Ave… 25th St                  40.7             -74.0
    ## 2 route2       <NA>       4 Ave… 25th St                  40.7             -74.0
    ## 3 route3       <NA>       4 Ave… 25th St                  40.7             -74.0
    ## 4 route4       <NA>       4 Ave… 25th St                  40.7             -74.0
    ## 5 route5       <NA>       4 Ave… 25th St                  40.7             -74.0
    ## 6 route6       <NA>       4 Ave… 25th St                  40.7             -74.0
    ## # ℹ 4 more variables: entry <lgl>, vending <lgl>, entrance_type <chr>,
    ## #   ada <lgl>

``` r
train_A = df_re |> filter(route_name=='A')
train_A
```

    ## # A tibble: 273 × 10
    ##    route_number route_name line  station_name station_latitude station_longitude
    ##    <chr>        <chr>      <chr> <chr>                   <dbl>             <dbl>
    ##  1 route1       A          42nd… Times Square             40.8             -74.0
    ##  2 route1       A          8 Av… 125th St                 40.8             -74.0
    ##  3 route1       A          8 Av… 125th St                 40.8             -74.0
    ##  4 route1       A          8 Av… 125th St                 40.8             -74.0
    ##  5 route1       A          8 Av… 125th St                 40.8             -74.0
    ##  6 route1       A          8 Av… 125th St                 40.8             -74.0
    ##  7 route1       A          8 Av… 125th St                 40.8             -74.0
    ##  8 route1       A          8 Av… 145th St                 40.8             -73.9
    ##  9 route1       A          8 Av… 145th St                 40.8             -73.9
    ## 10 route1       A          8 Av… 145th St                 40.8             -73.9
    ## # ℹ 263 more rows
    ## # ℹ 4 more variables: entry <lgl>, vending <lgl>, entrance_type <chr>,
    ## #   ada <lgl>

There are 273 stations serve train A and 107 among them are ADA
compliant.

# Problem 2

Import and combine the three datasets:

``` r
mr_trash_wheel = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", sheet = 1) |>
  janitor::clean_names() |> 
  select(-c('x15','x16')) |> 
  filter(is.na(dumpster)==FALSE) |> 
  mutate(sports_balls = as.integer(round(sports_balls)),
         wheel_type = "Mr. Trash Wheel")

prof_trash_wheel = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", sheet = 2) |>
  janitor::clean_names()  |> 
  filter(is.na(dumpster)==FALSE) |> 
  mutate(wheel_type = "Professor Trash Wheel",
         sports_balls= NA,
         glass_bottles= NA) |> 
  select(colnames(mr_trash_wheel))

gwy_trash_wheel = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", sheet = 4) |>
  janitor::clean_names() |> 
  filter(is.na(dumpster)==FALSE) |> 
  mutate(wheel_type = "Gwynnda Trash Wheel",
         sports_balls= NA,
         glass_bottles = NA,
         plastic_bags = NA)|> 
  select(colnames(mr_trash_wheel))

df_combine = rbind(mr_trash_wheel, prof_trash_wheel)
df_combine = rbind(df_combine, gwy_trash_wheel)
df_combine
```

    ## # A tibble: 1,033 × 15
    ##    dumpster month year  date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ##  1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ##  2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ##  3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ##  4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ##  5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ##  6        6 May   2014  2014-05-20 00:00:00        2.71                 13
    ##  7        7 May   2014  2014-05-21 00:00:00        1.91                  8
    ##  8        8 May   2014  2014-05-28 00:00:00        3.7                  16
    ##  9        9 June  2014  2014-06-05 00:00:00        2.52                 14
    ## 10       10 June  2014  2014-06-11 00:00:00        3.76                 18
    ## # ℹ 1,023 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, wheel_type <chr>

``` r
prof_trash = sum(prof_trash_wheel$weight_tons, na.rm=T)
gwy_cigg = gwy_trash_wheel |> filter(month=='June' & year == 2022)
gwy_cigg_num = sum(gwy_cigg$cigarette_butts)
```

### Description

Since the Professor Trash Wheel and Gwynnda datasets contain less
columns than Mr. Trash Wheel, I add NA columns to both of them in order
to make the three dataset have the same columns. Besides, to keep track
of which Trash Wheel is which, I add a column named `wheel_type` to each
of the three dataset. Finally, I combine the three dataset and the
resulting data contains 15 columns and 1033 rows.

To be more specific, the columns of the dataset are dumpster, month,
year, date, weight_tons, volume_cubic_yards, plastic_bottles,
polystyrene, cigarette_butts, glass_bottles, plastic_bags, wrappers,
sports_balls, homes_powered, wheel_type. It records the data of the
Trash Wheel did its work, how much trash it collected, which dumpster
they dumped the trash into and how much electricity these trash can make
for Maryland homes.

### Answer

- The total weight of trash collected by Professor Trash Wheel is
  246.74.

- The total number of cigarette butts collected by Gwynnda in June of
  2022 is 1.812^{4}.

# Problem 3

## Create a Well-organized Dataset

``` r
baker_df = read_csv("./data/gbb_datasets/bakers.csv") |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker", "last_name"), sep = " ")
bakes_df = read_csv("./data/gbb_datasets/bakes.csv")|> 
  janitor::clean_names() |> 
  mutate(baker = ifelse(baker=='"Jo"',"Jo",baker))
  
results_df = read_csv("./data/gbb_datasets/results.csv", skip=2)|> 
  janitor::clean_names()|> 
  mutate(baker = ifelse(baker=='"Jo"',"Jo",baker))

merge_df = merge(baker_df,bakes_df, by = intersect(colnames(bakes_df),colnames(baker_df)))
merge_df = merge(merge_df,results_df, by = intersect(colnames(merge_df),colnames(results_df)))

head(merge_df,5)
```

    ##   series   baker episode last_name baker_age baker_occupation      hometown
    ## 1      1 Annetha       1     Mills        30          Midwife         Essex
    ## 2      1 Annetha       2     Mills        30          Midwife         Essex
    ## 3      1   David       1  Chambers        31     Entrepreneur Milton Keynes
    ## 4      1   David       2  Chambers        31     Entrepreneur Milton Keynes
    ## 5      1   David       3  Chambers        31     Entrepreneur Milton Keynes
    ##                                         signature_bake
    ## 1 Light Jamaican Black Cakewith Strawberries and Cream
    ## 2                                Rose Petal Shortbread
    ## 3                                Chocolate Orange Cake
    ## 4           Cheddar Cheese and Fresh Rosemary Biscuits
    ## 5                                         Chilli Bread
    ##                                                                                                     show_stopper
    ## 1                                       Red, White & Blue Chocolate Cake with Cigarellos, Fresh Fruit, and Cream
    ## 2                                                                                  Pink Swirl Macarons / Eclairs
    ## 3 Black Forest Floor Gateauxwith Moulded Chocolate Leaves, Fallen Fruitand Chocolate Mushrooms Moulded from eggs
    ## 4                                                                     Choux Pastry Swans  Chocolate Profiteroles
    ## 5                                                                 Walnut and Seed Roll\nRed Berry and Almond Bun
    ##   technical result
    ## 1         2     IN
    ## 2         7    OUT
    ## 3         3     IN
    ## 4         8     IN
    ## 5         4     IN

``` r
write_csv(merge_df,"data/gbb_datasets/megered_data.csv")
```

I check each dataset and find that the baker_name recorded in bakers.csv
is the full name while that in bakes.csv and result.csv only includes
first name. So I use `seperate()` function to divides the baker name
into the first name and last name, which are named as `baker` and
`last name` respectively.

Additionally, the baker name contains a special case `"Jo"` in
bakes.csv, so I change `"Jo"` to `Jo` via `mutate()`. Finally, I merge
the three datasets together and the final dataset only includes the
observation (consists of series, baker and episode) that belongs to all
of the three datasets. My final dataset has 11 columns and 540 rows. The
columns include the baker’s name, age, occupation, hometown, signature
bake, show stopper, and also the series, episode and technical and the
result.

### Showing Baker Winner

The following table shows the winner or star baker of each episode in
Seasons 5 through 10.

``` r
winner_tab = results_df |> filter((result %in% c('WINNER','STAR BAKER')) & (series %in% 5:10)) |> 
  mutate(series=paste('series',series),
         episode= paste('episode',episode)) |> 
  select(c('series', 'episode', 'baker')) |> 
  pivot_wider(names_from = series, values_from = baker)
colnames(winner_tab)[1]<-'episode/series'
knitr::kable(winner_tab)
```

| episode/series | series 5 | series 6 | series 7  | series 8 | series 9 | series 10 |
|:---------------|:---------|:---------|:----------|:---------|:---------|:----------|
| episode 1      | Nancy    | Marie    | Jane      | Steven   | Manon    | Michelle  |
| episode 2      | Richard  | Ian      | Candice   | Steven   | Rahul    | Alice     |
| episode 3      | Luis     | Ian      | Tom       | Julia    | Rahul    | Michael   |
| episode 4      | Richard  | Ian      | Benjamina | Kate     | Dan      | Steph     |
| episode 5      | Kate     | Nadiya   | Candice   | Sophie   | Kim-Joy  | Steph     |
| episode 6      | Chetna   | Mat      | Tom       | Liam     | Briony   | Steph     |
| episode 7      | Richard  | Tamal    | Andrew    | Steven   | Kim-Joy  | Henry     |
| episode 8      | Richard  | Nadiya   | Candice   | Stacey   | Ruby     | Steph     |
| episode 9      | Richard  | Nadiya   | Andrew    | Sophie   | Ruby     | Alice     |
| episode 10     | Nancy    | Nadiya   | Candice   | Sophie   | Rahul    | David     |

I find that the WINNER in most series was the person who had won the
most times of ‘STAR BAKER’ in the previous 9 episodes in the same
series. However, there was a special case in series10. The WINNER in
series10 is David, who never won a ‘STAR BAKER’ or ‘WINNER’ in the
previous episodes.

### Viewership Data

``` r
viewer_df = read_csv("./data/gbb_datasets/viewers.csv") |> 
  janitor::clean_names() 
head(viewer_df,10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

The average viewership in Season 1 is 2.77, while the average viewership
in Season 5 is 10.0393.
